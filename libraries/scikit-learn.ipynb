{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn\n",
    "\n",
    "scikit-learn (lowercase) is the premier Python machine learning library.\n",
    "\n",
    "https://scikit-learn.org/\n",
    "\n",
    "See also:\n",
    "- https://github.com/phausamann/sklearn-xarray\n",
    "- https://github.com/nbren12/sklearn-xarray\n",
    "- https://github.com/dask/dask-ml/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn Tutorials\n",
    "\n",
    "- https://github.com/jakevdp/sklearn_tutorial/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn Overview\n",
    "\n",
    "Data arrays in scikit-learn are always 2-D shape `(n_samples, n_features)`, and input is typically cast to float64 (np).\n",
    "\n",
    "In scikit-learn, an estimator for classification is a Python object that implements the methods `fit(X, y)` and `predict(T)`.\n",
    "\n",
    "Fit the estimator instance to the model using the `fit()` method, then `predict()` new values.\n",
    "\n",
    "\n",
    "It appears that estimators have parameters which are normally set when creating an instance. They seem accesssible (settable?) as attributes and via `get_params()` and `set_params()`.\n",
    "\n",
    "Now PCA has `transform()`, and not a `predict()` method, so...? I think this means it's a Transformer. See [5. Dataset transformations].(https://scikit-learn.org/stable/data_transforms.html). But are. It seems `transform()` is related to dimensionality reduction, and some unsupervised learning methods have it also. The nuances of estimators vs transformers is vague, there is some indication that transformers are estimators, but what if they don't have predict?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.decomposition: Matrix Decomposition\n",
    "[sklearn.decomposition](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition)\n",
    "\n",
    "- [2.5. Decomposing signals in components (matrix factorization problems)](https://scikit-learn.org/stable/modules/decomposition.html)\n",
    "\n",
    "Algorithms include:\n",
    "- `IncrementalPCA`\n",
    "- `KernelPCA`\n",
    "- `PCA`\n",
    "-`TruncatedSVD`---does not \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "\n",
    "Parameters include:\n",
    "- n_components\n",
    "- whiten\n",
    "- svd_solver---`randomized`?\n",
    "\n",
    "Attributes include:\n",
    "- `components_`---sorted eigenvectors\n",
    "- `explained_variance_`---sorted eigenvalues\n",
    "- `explained_variance_ratio_`---\n",
    "\n",
    "Methods include:\n",
    "- `fit()`\n",
    "- `fit_transform()`\n",
    "- `get_covariance()`\n",
    "- `transform()`\n",
    "\n",
    "If copy=False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html\n",
    "\n",
    "Useful resources:\n",
    "- https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.preprocessing: Preprocessing and Normalization\n",
    "\n",
    "The [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) module\n",
    "\n",
    "- [5.3 Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "Includes:\n",
    "- `StandardScalar`---standardize features by removing the mean and scaling to unit variance (divide by standard deviation), use like `StandardScalar().fit_transform(X)`. Compare to `scale()`. Assumes normal distribution.\n",
    "- `Normalizer`---Compare to `normalize()`. Divides each value by magnitude. Applied to rows/observations, not columns/features.\n",
    "- `MinMaxScaler`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here's the questions, how do you reproject when you run PCA on standardized data? Do you transform the standardized data or the original? It isn't obvious but I think you project the transformed data.\n",
    "\n",
    "X_std = StandardScalar().fit_transform(X)\n",
    "X_pca = PCA().fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.neighbors: Nearest Neighbors\n",
    "\n",
    "- [1.6 Nearest Neighbors](https://scikit-learn.org/stable/modules/neighbors.html)\n",
    "- [2.8 Density Estimation](https://scikit-learn.org/stable/modules/density.html)\n",
    "\n",
    "Includes:\n",
    "- `KernelDensity`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
